{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc413 project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPwrJqFqmwH4"
      },
      "source": [
        "# **Code setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ncKPXXvom09",
        "outputId": "5bbb01e1-658d-48e6-c1ce-ca230df77555"
      },
      "source": [
        "! pip install pretty_midi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretty_midi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/8e/63c6e39a7a64623a9cd6aec530070c70827f6f8f40deec938f323d7b1e15/pretty_midi-0.2.9.tar.gz (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n",
            "Collecting mido>=1.1.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-cp37-none-any.whl size=5591954 sha256=01eb090a608135ed7800a73bb4d7292722dc5e522f20e57307716067cf3c7b48\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/a1/c6/b5697841db1112c6e5866d75a6b6bf1bef73b874782556ba66\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.9 pretty-midi-0.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9siuv2qnCKG"
      },
      "source": [
        "# imports \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from random import shuffle\n",
        "from pretty_midi import Note, PrettyMIDI, Instrument, ControlChange\n",
        "import six\n",
        "import copy, pathlib\n",
        "import os, time, datetime, random, copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcgZmXdknY1I"
      },
      "source": [
        "# helper functions\n",
        "\n",
        "def vectorize(sequence):\n",
        "    \"\"\"\n",
        "    Converts a list of pretty_midi Note objects into a numpy array of\n",
        "    dimension (n_notes x 4)\n",
        "    \"\"\"\n",
        "    array = [[note.start, note.end, note.pitch, note.velocity] for\n",
        "            note in sequence]\n",
        "    return np.asarray(array)\n",
        "\n",
        "def devectorize(note_array):\n",
        "    \"\"\"\n",
        "    Converts a vectorized note sequence into a list of pretty_midi Note\n",
        "    objects\n",
        "    \"\"\"\n",
        "    return [Note(start = a[0], end = a[1], pitch=a[2],\n",
        "        velocity=a[3]) for a in note_array.tolist()]\n",
        "\n",
        "\n",
        "def one_hot(sequence, n_states):\n",
        "    \"\"\"\n",
        "    Given a list of integers and the maximal number of unique values found\n",
        "    in the list, return a one-hot encoded tensor of shape (m, n)\n",
        "    where m is sequence length and n is n_states.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.eye(n_states)[sequence,:].cuda()\n",
        "    else:\n",
        "        return torch.eye(n_states)[sequence,:]\n",
        "\n",
        "def decode_one_hot(vector):\n",
        "    '''\n",
        "    Given a one-hot encoded vector, return the non-zero index\n",
        "    '''\n",
        "    return vector.nonzero().item()\n",
        "\n",
        "def prepare_batches(sequences, batch_size):\n",
        "    \"\"\"\n",
        "    Splits a list of sequences into batches of a fixed size. Each sequence yields an input sequence\n",
        "    and a target sequence, with the latter one time step ahead. For example, the sequence \"to be or not\n",
        "    to be\" gives an input sequence of \"to be or not to b\" and a target sequence of \"o be or not to be.\"\n",
        "    \"\"\"\n",
        "    n_sequences = len(sequences)\n",
        "    for i in range(0, n_sequences, batch_size):\n",
        "        batch = sequences[i:i+batch_size]\n",
        "\t#needs to be in sorted order for packing batches to work\n",
        "        batch = sorted(batch, key = len, reverse=True)\n",
        "        input_sequences, target_sequences = [], []\n",
        "\n",
        "        for sequence in batch:\n",
        "            input_sequences.append(sequence[:-1])\n",
        "            target_sequences.append(sequence[1:])\n",
        "\n",
        "        yield input_sequences, target_sequences\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Clone N identical layers of a module\"\n",
        "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "def d(tensor=None):\n",
        "    if tensor is None:\n",
        "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
        "\n",
        "def write_midi(note_sequence, output_dir, filename):\n",
        "\n",
        "    #make output directory\n",
        "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    #generate midi\n",
        "    midi = PrettyMIDI()\n",
        "    piano_track = Instrument(program=0, is_drum=False, name=filename)\n",
        "    piano_track.notes = note_sequence\n",
        "    midi.instruments.append(piano_track)\n",
        "    output_name = output_dir + f\"{filename}.midi\"\n",
        "    midi.write(output_name)\n",
        "\n",
        "def sample(model, sample_length, prime_sequence=[], temperature=1):\n",
        "    \"\"\"\n",
        "    Generate a MIDI event sequence of a fixed length by randomly sampling from a model's distribution of sequences. Optionally, \"seed\" the sequence with a prime. A well-trained model will create music that responds to the prime and develops upon it.\n",
        "    \"\"\"\n",
        "    #deactivate training mode\n",
        "    model.eval()\n",
        "    if len(prime_sequence) == 0:\n",
        "        #if no prime is provided, randomly select a starting event\n",
        "        input_sequence = [np.random.randint(model.n_tokens)]\n",
        "    else:\n",
        "        input_sequence = prime_sequence.copy()\n",
        "\n",
        "    #add singleton dimension for the batch\n",
        "    input_tensor = torch.LongTensor(input_sequence).unsqueeze(0)\n",
        "\n",
        "    for i in range(sample_length):\n",
        "        #select probabilities of *next* token\n",
        "        out = model(input_tensor)[0, -1, :]\n",
        "        #out is a 1d tensor of shape (n_tokens)\n",
        "        probs = F.softmax(out / temperature, dim=0)\n",
        "        #sample prob distribution for next character\n",
        "        c = torch.multinomial(probs,1)\n",
        "        input_tensor = torch.cat([input_tensor[:,1:], c[None]], dim=1)\n",
        "        input_sequence.append(c.item())\n",
        "\n",
        "    return input_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBTxCryznDwv"
      },
      "source": [
        "# custom help code for training\n",
        "class Accuracy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, prediction, target, mask=None, token_dim=-1,\n",
        "            sequence_dim=-2):\n",
        "\n",
        "        #normalize by token classes and guess most probable sequence\n",
        "        prediction = F.softmax(prediction, token_dim)\\\n",
        "                .argmax(sequence_dim)\n",
        "\n",
        "\n",
        "        scores = (prediction == target)\n",
        "        n_padded = 0\n",
        "        if mask is not None:\n",
        "            n_padded = (mask == 0).sum()\n",
        "        return scores.sum() / float(scores.numel() - n_padded)\n",
        "\n",
        "\n",
        "def smooth_cross_entropy(prediction, target, eps=0.1,\n",
        "        ignore_index=0):\n",
        "\n",
        "\n",
        "    mask = (target == ignore_index).unsqueeze(-1)\n",
        "\n",
        "    prediction = prediction.transpose(1,2)\n",
        "    n_classes = prediction.shape[-1]\n",
        "    #one hot encode target\n",
        "    p = F.one_hot(target, n_classes)\n",
        "    #uniform distribution probability\n",
        "    u = 1.0 / n_classes\n",
        "    p_prime = (1.0 - eps) * p + eps * u\n",
        "    #ignore padding indices\n",
        "    p_prime = p_prime.masked_fill(mask, 0)\n",
        "    #cross entropy\n",
        "    h = -torch.sum(p_prime * F.log_softmax(prediction, -1))\n",
        "    #mean reduction\n",
        "    n_items = torch.sum(target != ignore_index)\n",
        "\n",
        "    return h / n_items\n",
        "\n",
        "\n",
        "class TFSchedule:\n",
        "    \"\"\"\n",
        "    From https://www.tensorflow.org/tutorials/text/transformer. Wrapper for Optimizer, gradually increases learning rate for a warmup period before learning rate decay sets in.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
        "\n",
        "        self.opt = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "        self._step = 0\n",
        "        self._rate = 0\n",
        "\n",
        "    def step(self):\n",
        "\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.opt.param_groups:\n",
        "            p['lr'] = rate\n",
        "\n",
        "        self._rate = rate\n",
        "        self.opt.step()\n",
        "\n",
        "\n",
        "    def rate(self, step=None):\n",
        "\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "\n",
        "        arg1 = step ** (-0.5)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return self.d_model ** (-0.5) * min(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCHVe_3Rn-SK"
      },
      "source": [
        "# sequence encoding code\n",
        "class SequenceEncoderError(Exception):\n",
        "    pass\n",
        "\n",
        "class SequenceEncoder():\n",
        "    \"\"\"\n",
        "    Converts sequences of Midi Notes to sequences of events under the following \n",
        "    representation:\n",
        "    - 128 NOTE-ON events (for each of the 128 MIDI pitches, starts a new note)\n",
        "    - 128 NOTE-OFF events (likewise. Ends \n",
        "    - (1000 / t) TIME-SHIFT events (each moves the time step forward by increments of \n",
        "      t ms up to 1 second\n",
        "    - v VELOCITY events (each one changes the velocity applied to all subsequent notes\n",
        "      until another velocity event occurs)\n",
        "    Includes functions to cast a sequence of Midi Notes to a numeric list of \n",
        "    possible events and to one-hot encode a numeric sequence as a Pytorch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_time_shift_events, n_velocity_events,\n",
        "            sequences_per_update=50000, min_events=33, max_events=513):\n",
        "        self.n_time_shift_events = n_time_shift_events\n",
        "        self.n_events = 256 + n_time_shift_events + n_velocity_events\n",
        "        self.timestep = 1 / n_time_shift_events\n",
        "        self.velocity_bin_size = 128 // n_velocity_events\n",
        "        self.sequences_per_update = sequences_per_update\n",
        "        self.min_events = min_events\n",
        "        self.max_events = max_events\n",
        "\n",
        "    def encode_sequences(self, sample_sequences):\n",
        "        \"\"\"\n",
        "        Converts each sample note sequence into an \"event\" sequence, a list of integers\n",
        "        0 through N-1 where N is the total number of events in the encoder's\n",
        "        representation.\n",
        "        \"\"\"\n",
        "        event_sequences = []\n",
        "        #count how many sequences are discarded/truncated due to length\n",
        "        short_count, long_count = 0,0\n",
        "        n_sequences = len(sample_sequences)\n",
        "        for i in range(n_sequences):\n",
        "            if not (i % self.sequences_per_update):\n",
        "                print(\"{:,} / {:,} sequences encoded\".\\\n",
        "                        format(i, n_sequences))\n",
        "            event_sequence = []\n",
        "            event_timestamps = []\n",
        "            #attempt at efficiency gain: only add a velocity event if it's different\n",
        "            #from current velocity...this is tricky if two notes played at the\n",
        "            #same time have different velocity\n",
        "            #current_velocity = 0\n",
        "            for note in sample_sequences[i]:\n",
        "                #extract start/end time, pitch and velocity\n",
        "                t0, t1, p, v = note\n",
        "                event_timestamps.append((t0, \"VELOCITY\", v))\n",
        "                #if v != current_velocity:\n",
        "                #    event_timestamps.append((t0, \"VELOCITY\", v))\n",
        "                #    current_velocity = v\n",
        "                event_timestamps.append((t0, \"NOTE_ON\", p))\n",
        "                event_timestamps.append((t1, \"NOTE_OFF\", p))\n",
        "\n",
        "            # sort events by timestamp\n",
        "            event_timestamps = sorted(event_timestamps, key = lambda x: x[0])\n",
        "            current_time = 0\n",
        "            max_timeshift = self.n_time_shift_events\n",
        "            #this loop encodes timeshifts as numbers\n",
        "            #consider turning this into a function to help readability\n",
        "            for timestamp in event_timestamps:\n",
        "                #capture a shift in absolute time\n",
        "                if timestamp[0] != current_time:\n",
        "                    #convert to relative time and convert to number of quantized timesteps\n",
        "                    timeshift = (timestamp[0] *  self.n_time_shift_events) - \\\n",
        "                            (current_time * self.n_time_shift_events)\n",
        "                    #this is hacky but sue me\n",
        "                    timeshift = int(timeshift + .1)\n",
        "                    timeshift_events = []\n",
        "                    #aggregate pauses longer than one second, as necessary\n",
        "                    while timeshift > max_timeshift:\n",
        "                        timeshift_events.append(\n",
        "                                self.event_to_number(\"TIME_SHIFT\", max_timeshift))\n",
        "                        timeshift -= max_timeshift\n",
        "                    #add timeshift (mod 1 second) as an event\n",
        "                    timeshift_events.append(\n",
        "                            self.event_to_number(\"TIME_SHIFT\", timeshift))\n",
        "                    event_sequence.extend(timeshift_events)\n",
        "                    \n",
        "                    #add the other events: NOTE_ON, NOTE_OFF, VELOCITY\n",
        "                    current_time = timestamp[0]\n",
        "                event_sequence.append(\n",
        "                        self.event_to_number(timestamp[1], timestamp[2]))\n",
        "\n",
        "            #check if sequence is too short to keep\n",
        "            if self.min_events is not None:\n",
        "                if len(event_sequence) < self.min_events:\n",
        "                    short_count += 1\n",
        "                    continue\n",
        "            #truncate sequence if necessary\n",
        "            if self.max_events is not None:\n",
        "                if len(event_sequence) > self.max_events:\n",
        "                    event_sequence = event_sequence[:self.max_events]\n",
        "                    long_count += 1\n",
        "\n",
        "            event_sequences.append(event_sequence)\n",
        "\n",
        "        if short_count > 0:\n",
        "            print(f\"{short_count} sequences discarded due to brevity\")\n",
        "        if long_count > 0:\n",
        "            print(f\"{long_count} sequences truncated due to excessive length.\")\n",
        "\n",
        "        return event_sequences\n",
        "\n",
        "                \n",
        "    def event_to_number(self, event, value):\n",
        "        \"\"\"\n",
        "        Encode an event/value pair as a number 0-N-1\n",
        "        where N is the number of unique events in the Encoder's representation.\n",
        "        \"\"\"\n",
        "        if event == \"NOTE_ON\":\n",
        "            return value\n",
        "        elif event == \"NOTE_OFF\":\n",
        "            return value + 128\n",
        "        elif event == \"TIME_SHIFT\":\n",
        "            #subtract one to fit to zero-index convention\n",
        "            #i.e. the number 256 corresponds to the smallest possible timestep\n",
        "            #which is non-zero...!\n",
        "            return value + 256 - 1\n",
        "        elif event == \"VELOCITY\":\n",
        "            #convert to bins\n",
        "            v_bin = (value - 1) // self.velocity_bin_size\n",
        "            return v_bin + 256 + self.n_time_shift_events\n",
        "        else:\n",
        "            raise SequenceEncoderError(\"Event type {} not recognized\".format(event))\n",
        "\n",
        "    def number_to_event(self, number):\n",
        "        number = int(number)\n",
        "        if number < 0 or number >= self.n_events:\n",
        "            raise SequenceEncoderError(\"Number {} out of range\")\n",
        "\n",
        "        if number < 128:\n",
        "            event = \"NOTE_ON\", number\n",
        "        elif 128 <= number < 256:\n",
        "            event = \"NOTE_OFF\", number - 128\n",
        "        elif 256 <= number < 256 + self.n_time_shift_events:\n",
        "            event = \"TIME_SHIFT\", number + 1 - 256\n",
        "        else:\n",
        "            bin_number = number - 256 - self.n_time_shift_events\n",
        "            event = \"VELOCITY\", (bin_number * self.velocity_bin_size) + 1\n",
        "        return event\n",
        "\n",
        "    def decode_sequences(self, encoded_sequences):\n",
        "        \"\"\"\n",
        "        Given a list of encoded sequences, decode each of them and return a list of pretty_midi Note sequences.\n",
        "        \"\"\"\n",
        "        note_sequences = []\n",
        "        for encoded_sequence in encoded_sequences:\n",
        "            note_sequences.append(self.decode_sequence(encoded_sequence))\n",
        "\n",
        "        return note_sequences\n",
        "\n",
        "    def decode_sequence(self, encoded_sequence, stuck_note_duration=None, keep_ghosts=False, verbose=False):\n",
        "        \"\"\"\n",
        "        Takes in an encoded event sequence (sparse numerical representation) and transforms it back into a pretty_midi Note sequence. Randomly-generated encoded sequences, such as produced by the generation script, can have some unusual traits such as notes without a provided end time. Contains logic to handle these pathological notes.\n",
        "        Args:\n",
        "            encoded_sequence (list): List of events encoded as integers\n",
        "            stuck_note_duration (int or None): if defined, for recovered notes missing an endtime, give them a fixed duration (as number of seconds held)\n",
        "            keep_ghosts (bool): if true, when the decoding algorithm recovers notes with an end time preceding their start time, keep them by swapping start and end. If false, discard the \"ghost\" notes\n",
        "            verbose (bool): If true, print results on how many stuck notes and ghost notes are detected.\n",
        "        \"\"\"\n",
        "        events = []\n",
        "        for num in encoded_sequence:\n",
        "            events.append(self.number_to_event(num))\n",
        "        #list of pseudonotes = {'start':x, 'pitch':something, 'velocity':something}\n",
        "        notes = []\n",
        "        #on the second pass, add in end time\n",
        "        note_ons = []\n",
        "        note_offs = []\n",
        "        global_time = 0\n",
        "        current_velocity = 0\n",
        "        for event, value in events:\n",
        "            #check event type\n",
        "            if event == \"TIME_SHIFT\":\n",
        "                global_time += 0.008 * value\n",
        "                global_time = round(global_time, 5)\n",
        "\n",
        "            elif event == \"VELOCITY\":\n",
        "                current_velocity = value\n",
        "            \n",
        "            elif event == \"NOTE_OFF\":\n",
        "                #eventually we'll sort this by timestamp and work thru\n",
        "                note_offs.append({\"pitch\": value, \"end\": global_time})\n",
        "            \n",
        "            elif event == \"NOTE_ON\":\n",
        "                #it's a NOTE_ON!\n",
        "                #value is pitch \n",
        "                note_ons.append({\"start\": global_time, \n",
        "                    \"pitch\": value, \"velocity\": current_velocity})\n",
        "            else:\n",
        "                raise SequenceEncoderError(\"you fool!\")\n",
        "\n",
        "        #keep a count of notes that are missing an end time (stuck notes)\n",
        "        #----default behavior is to ignore them. \n",
        "        stuck_notes = 0\n",
        "        \n",
        "        #keep a count of notes assigned end times *before* their start times (ghost notes)\n",
        "        #----default behavior is to ignore them\n",
        "        ghost_notes = 0\n",
        "\n",
        "\n",
        "        #Zip up notes with corresponding note-off events\n",
        "        while len(note_ons) > 0:\n",
        "            note_on = note_ons[0]\n",
        "            pitch = note_on['pitch']\n",
        "            #this assumes everything is sorted nicely!\n",
        "            note_off = next((n for n in note_offs if n['pitch'] == pitch), None)\n",
        "            if note_off == None:\n",
        "                stuck_notes += 1\n",
        "                if stuck_note_duration is None:\n",
        "                    note_ons.remove(note_on)\n",
        "                    continue\n",
        "                else:\n",
        "                    note_off = {\"pitch\": pitch, \"end\": note_on['start'] + stuck_note_duration}\n",
        "            else:\n",
        "                note_offs.remove(note_off)\n",
        "\n",
        "            if note_off['end'] < note_on['start']:\n",
        "                ghost_notes += 1\n",
        "                if keep_ghosts:\n",
        "                    #reverse start and end (and see what happens...!)\n",
        "                    new_end = note_on['start']\n",
        "                    new_start = note_off['end']\n",
        "                    note_on['start'] = new_start\n",
        "                    note_off['end'] = new_end\n",
        "                else:\n",
        "                    note_ons.remove(note_on)\n",
        "                    continue\n",
        "\n",
        "            note = Note(start = note_on['start'], end = note_off['end'],\n",
        "                    pitch = pitch, velocity = note_on['velocity'])\n",
        "            notes.append(note)\n",
        "            note_ons.remove(note_on)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"{stuck_notes} notes missing an end-time...\")\n",
        "            print(f\"{ghost_notes} had an end-time precede their start-time\")\n",
        "\n",
        "        return notes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FouA7XYnO2u"
      },
      "source": [
        "# preprocessing pipeline\n",
        "\n",
        "class PreprocessingError(Exception):\n",
        "    pass\n",
        "\n",
        "class PreprocessingPipeline():\n",
        "    #set a random seed\n",
        "    SEED = 1811\n",
        "    \"\"\"\n",
        "    Pipeline to convert MIDI files to cleaned Piano Midi Note Sequences, split into \n",
        "    a more manageable length.\n",
        "    Applies any sustain pedal activity to extend note lengths. Optionally augments\n",
        "    the data by transposing pitch and/or stretching sample speed. Optionally quantizes\n",
        "    timing and/or dynamics into smaller bins.\n",
        "    Attributes:\n",
        "        self.split_samples (dict of lists): when the pipeline is run, has two keys, \"training\" and \"validation,\" each holding a list of split MIDI note sequences.\n",
        "        self.encoded_sequences (dict of lists): Keys are \"training\" and \"validation.\" Each holds a list of encoded event sequences, a sparse numeric representation of a MIDI sample.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dir, stretch_factors = [0.95, 0.975, 1, 1.025, 1.05],\n",
        "            split_size = 30, sampling_rate = 125, n_velocity_bins = 32,\n",
        "            transpositions = range(-3,4), training_val_split = 0.9, \n",
        "            max_encoded_length = 512, min_encoded_length = 33):\n",
        "        self.input_dir = input_dir\n",
        "        self.split_samples = dict()\n",
        "        self.stretch_factors = stretch_factors\n",
        "        #size (in seconds) in which to split midi samples\n",
        "        self.split_size = split_size\n",
        "        #In hertz (beats per second), quantize sample timings to this discrete frequency\n",
        "        #So a sampling rate of 125 hz means a smallest time steps of 8 ms\n",
        "        self.sampling_rate = sampling_rate\n",
        "        #Quantize sample dynamics (Velocity 1-127) to a smaller number of bins\n",
        "        #this should be an *integer* dividing 128 cleanly: 2,4,8,16,32,64, or 128. \n",
        "        self.n_velocity_bins = n_velocity_bins\n",
        "        self.transpositions = transpositions\n",
        "        \n",
        "        #Fraction of raw MIDI data that goes to the training set\n",
        "        #the remainder goes to validat\n",
        "        self.training_val_split = training_val_split\n",
        "\n",
        "        self.encoder = SequenceEncoder(n_time_shift_events = sampling_rate,\n",
        "                n_velocity_events = n_velocity_bins, \n",
        "                min_events = min_encoded_length,\n",
        "                max_events = max_encoded_length)\n",
        "        self.encoded_sequences = dict()\n",
        "\n",
        "        random.seed(PreprocessingPipeline.SEED)\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dir (str): path to input directory. All .midi or .mid files in this directory will get processed.\n",
        "            stretch_factors (list of float): List of constants by which note end times and start times will be multiplied. A way to augment data.\n",
        "            split_size (int): Max length, in seconds, of samples into which longer MIDI note sequences are split.\n",
        "            sampling_rate (int): How many subdivisions of 1,000 milliseconds to quantize note timings into. E.g. a sampling rate of 100 will mean end and start times are rounded to the nearest 0.01 second.\n",
        "            n_velocity_bins (int): Quantize 128 Midi velocities (amplitudes) into this many bins: e.g. 32 velocity bins mean note velocities are rounded to the nearest multiple of 4.\n",
        "            transpositions (iterator of ints): Transpose note pitches up/down by intervals (number of half steps) in this iterator. Augments a dataset with transposed copies.\n",
        "            training_val_split (float): Number between 0 and 1 defining the proportion of raw data going to the training set. The rest goes to validation.\n",
        "            max_encoded_length (int): Truncate encoded samples containing more\n",
        "            events than this number.\n",
        "            min_encoded_length (int): Discard encoded samples containing fewer events than this number.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Main pipeline call...parse midis, split into test and validation sets,\n",
        "        augment, quantize, sample, and encode as event sequences. \n",
        "        \"\"\"\n",
        "        midis = self.parse_files(chdir=True) \n",
        "        total_time = sum([m.get_end_time() for m in midis])\n",
        "        print(\"\\n{} midis read, or {:.1f} minutes of music\"\\\n",
        "                .format(len(midis), total_time/60))\n",
        "\n",
        "        note_sequences = self.get_note_sequences(midis)\n",
        "        del midis\n",
        "        #vectorize note sequences\n",
        "        note_sequences = [vectorize(ns) for ns in note_sequences]\n",
        "        print(\"{} note sequences extracted\\n\".format(len(note_sequences)))\n",
        "        self.note_sequences = self.partition(note_sequences)\n",
        "        for mode, sequences in self.note_sequences.items():\n",
        "            print(f\"Processing {mode} data...\")\n",
        "            print(f\"{len(sequences):,} note sequences\")\n",
        "            if mode == \"training\":\n",
        "                sequences = self.stretch_note_sequences(sequences)\n",
        "                print(f\"{len(sequences):,} stretched note sequences\")\n",
        "            samples = self.split_sequences(sequences)\n",
        "            self.quantize(samples)\n",
        "            print(f\"{len(samples):,} quantized, split samples\")\n",
        "            if mode == \"training\":\n",
        "                samples = self.transpose_samples(samples)\n",
        "                print(f\"{len(samples):,} transposed samples\")\n",
        "            self.split_samples[mode] = samples\n",
        "            self.encoded_sequences[mode] = self.encoder.encode_sequences(samples)\n",
        "            print(f\"Encoded {mode} sequences!\\n\")\n",
        "\n",
        "    def parse_files(self, chdir=False):\n",
        "        \"\"\"\n",
        "        Recursively parse all MIDI files in a given directory to \n",
        "        PrettyMidi objects.\n",
        "        \"\"\"\n",
        "        if chdir: \n",
        "            home_dir = os.getcwd()\n",
        "            os.chdir(self.input_dir)\n",
        "\n",
        "        pretty_midis = []\n",
        "        folders = [d for d in os.listdir(os.getcwd()) if os.path.isdir(d)]\n",
        "        if len(folders) > 0:\n",
        "            for d in folders:\n",
        "                os.chdir(d)\n",
        "                pretty_midis += self.parse_files()\n",
        "                os.chdir(\"..\")\n",
        "        midis = [f for f in os.listdir(os.getcwd()) if \\\n",
        "                (f.endswith(\".mid\") or f.endswith(\"midi\"))]\n",
        "        print(f\"Parsing {len(midis)} midi files in {os.getcwd()}...\")\n",
        "        for m in midis:\n",
        "            with open(m, \"rb\") as f:\n",
        "                try:\n",
        "                    midi_str = six.BytesIO(f.read())\n",
        "                    pretty_midis.append(pretty_midi.PrettyMIDI(midi_str))\n",
        "                    #print(\"Successfully parsed {}\".format(m))\n",
        "                except:\n",
        "                    print(\"Could not parse {}\".format(m))\n",
        "        if chdir:\n",
        "            os.chdir(home_dir)\n",
        "\n",
        "        return pretty_midis\n",
        "\n",
        "    def get_note_sequences(self, midis):\n",
        "        \"\"\"\n",
        "        Given a list of PrettyMidi objects, extract the Piano track as a list of \n",
        "        Note objects. Calls the \"apply_sustain\" method to extract the sustain pedal\n",
        "        control changes.\n",
        "        \"\"\"\n",
        "\n",
        "        note_sequences = []\n",
        "        for m in midis:\n",
        "            if m.instruments[0].program == 0:\n",
        "                piano_data = m.instruments[0]\n",
        "            else:\n",
        "                #todo: write logic to safely catch if there are non piano instruments,\n",
        "                #or extract the piano midi if it exists\n",
        "                raise PreprocessingError(\"Non-piano midi detected\")\n",
        "            note_sequence = self.apply_sustain(piano_data)\n",
        "            note_sequence = sorted(note_sequence, key = lambda x: (x.start, x.pitch))\n",
        "            note_sequences.append(note_sequence)\n",
        "\n",
        "        return note_sequences\n",
        "\n",
        "\n",
        "\n",
        "    def apply_sustain(self, piano_data):\n",
        "        \"\"\"\n",
        "        While the sustain pedal is applied during a midi, extend the length of all \n",
        "        notes to the beginning of the next note of the same pitch or to \n",
        "        the end of the sustain. Returns a midi notes sequence.\n",
        "        \"\"\"\n",
        "        _SUSTAIN_ON = 0\n",
        "        _SUSTAIN_OFF = 1\n",
        "        _NOTE_ON = 2\n",
        "        _NOTE_OFF = 3\n",
        " \n",
        "        notes = copy.deepcopy(piano_data.notes)\n",
        "        control_changes = piano_data.control_changes\n",
        "        #sequence of SUSTAIN_ON, SUSTAIN_OFF, NOTE_ON, and NOTE_OFF actions\n",
        "        first_sustain_control = next((c for c in control_changes if c.number == 64),\n",
        "                ControlChange(number=64, value=0, time=0))\n",
        "\n",
        "        if first_sustain_control.value >= 64:\n",
        "            sustain_position = _SUSTAIN_ON\n",
        "        else:\n",
        "            sustain_position = _SUSTAIN_OFF\n",
        "        #if for some reason pedal was not touched...\n",
        "        action_sequence = [(first_sustain_control.time, sustain_position, None)]\n",
        "        #delete this please\n",
        "        cleaned_controls = []\n",
        "        for c in control_changes:\n",
        "            #Ignoring the sostenuto and damper pedals due to complications\n",
        "            if sustain_position == _SUSTAIN_ON:\n",
        "                if c.value >= 64:\n",
        "                    #another SUSTAIN_ON\n",
        "                    continue\n",
        "                else:\n",
        "                    sustain_position = _SUSTAIN_OFF\n",
        "            else:\n",
        "                #look for the next on signal\n",
        "                if c.value < 64:\n",
        "                    #another SUSTAIN_OFF\n",
        "                    continue\n",
        "                else:\n",
        "                    sustain_position = _SUSTAIN_ON\n",
        "            action_sequence.append((c.time, sustain_position, None))\n",
        "            cleaned_controls.append((c.time, sustain_position))\n",
        "    \n",
        "        action_sequence.extend([(note.start, _NOTE_ON, note) for note in notes])\n",
        "        action_sequence.extend([(note.end, _NOTE_OFF, note) for note in notes])\n",
        "        #sort actions by time and type\n",
        "    \n",
        "        action_sequence = sorted(action_sequence, key = lambda x: (x[0], x[1]))\n",
        "        live_notes = []\n",
        "        sustain = False\n",
        "        for action in action_sequence:\n",
        "            if action[1] == _SUSTAIN_ON:\n",
        "                sustain = True\n",
        "            elif action[1] == _SUSTAIN_OFF:\n",
        "                #find when the sustain pedal is released\n",
        "                off_time = action[0]\n",
        "                for note in live_notes:\n",
        "                    if note.end < off_time:\n",
        "                        #shift the end of the note to when the pedal is released\n",
        "                        note.end = off_time\n",
        "                        live_notes.remove(note)\n",
        "                sustain = False\n",
        "            elif action[1] == _NOTE_ON:\n",
        "                current_note = action[2]\n",
        "                if sustain:\n",
        "                    for note in live_notes:\n",
        "                        # if there are live notes of the same pitch being held, kill 'em\n",
        "                        if current_note.pitch == note.pitch:\n",
        "                            note.end = current_note.start\n",
        "                            live_notes.remove(note)\n",
        "                live_notes.append(current_note)\n",
        "            else:\n",
        "                if sustain == True:\n",
        "                    continue\n",
        "                else:\n",
        "                    note = action[2]\n",
        "                    try:\n",
        "                        live_notes.remove(note)\n",
        "                    except ValueError:\n",
        "                        print(\"***Unexpected note sequence...possible duplicate?\")\n",
        "                        pass\n",
        "        return notes\n",
        "\n",
        "    def partition(self, sequences):\n",
        "       \"\"\"\n",
        "       Partition a list of Note sequences into a training set and validation set.\n",
        "       Returns a dictionary {\"training\": training_data, \"validation\": validation_data}\n",
        "       \"\"\"\n",
        "       partitioned_sequences = {}\n",
        "       random.shuffle(sequences)\n",
        "\n",
        "       n_training = int(len(sequences) * self.training_val_split)\n",
        "       partitioned_sequences['training'] = sequences[:n_training]\n",
        "       partitioned_sequences['validation'] = sequences[n_training:]\n",
        "\n",
        "       return partitioned_sequences\n",
        "\n",
        "    def stretch_note_sequences(self, note_sequences):\n",
        "        \"\"\"\n",
        "        Stretches tempo (note start and end time) for each sequence in a given list\n",
        "        by each of the pipeline's stretch factors. Returns a list of Note sequences.\n",
        "        \"\"\"\n",
        "        stretched_note_sequences = []\n",
        "        for note_sequence in note_sequences:\n",
        "            for factor in self.stretch_factors:\n",
        "                if factor == 1:\n",
        "                    stretched_note_sequences.append(note_sequence)\n",
        "                    continue\n",
        "                stretched_sequence = np.copy(note_sequence)\n",
        "                #stretch note start time\n",
        "                stretched_sequence[:,0] *= factor\n",
        "                #stretch note end time\n",
        "                stretched_sequence[:,1] *= factor\n",
        "                stretched_note_sequences.append(stretched_sequence)\n",
        "\n",
        "        return stretched_note_sequences\n",
        "\n",
        "\n",
        "    def split_sequences(self, sequences):\n",
        "        \"\"\"\n",
        "        Given a list of Note sequences, splits them into samples no longer than \n",
        "        a given length. Returns a list of split samples.\n",
        "        \"\"\"\n",
        "\n",
        "        samples = []\n",
        "        if len(sequences) == 0:\n",
        "            raise PreprocessingError(\"No note sequences available to split\")\n",
        "\n",
        "        for note_sequence in sequences:\n",
        "            sample_length = 0\n",
        "            sample = []\n",
        "            i = 0\n",
        "            while i < len(note_sequence):\n",
        "                note = np.copy(note_sequence[i])\n",
        "                if sample_length == 0:\n",
        "                    sample_start = note[0]\n",
        "                    if note[1] > self.split_size + sample_start:\n",
        "                        #prevent case of a zero-length sample\n",
        "                        #print(f\"***Current note has length of more than {self.split_size} seconds...reducing duration\")\n",
        "                        note[1] = sample_start + self.split_size\n",
        "                    sample.append(note)\n",
        "                    sample_length = self.split_size\n",
        "                else:\n",
        "                    if note[1] <= sample_start + self.split_size:\n",
        "                        sample.append(note)\n",
        "                        if note[1] > sample_start + sample_length:\n",
        "                            sample_length = note[1] - sample_start\n",
        "                    else:\n",
        "                        samples.append(np.asarray(sample))\n",
        "                        #sample start should begin with the beginning of the\n",
        "                        #*next* note, how do I handle this...\n",
        "                        sample_length = 0\n",
        "                        sample = []\n",
        "                i += 1\n",
        "        return samples\n",
        "\n",
        "    def quantize(self, samples):\n",
        "        \"\"\"\n",
        "        Quantize timing and dynamics in a Note sample in place. This converts continuous\n",
        "        time to a discrete, encodable quantity and simplifies input for the model.\n",
        "        Quantizes note start/ends to a smallest perceptible timestep (~8ms) and note\n",
        "        velocities to a few audibly distinct bins (around 32).\n",
        "        \"\"\"\n",
        "        #define smallest timestep (in seconds)\n",
        "        try:\n",
        "            timestep = 1 / self.sampling_rate\n",
        "        except ZeroDivisionError:\n",
        "            timestep = 0\n",
        "        #define smallest dynamics increment\n",
        "        try:\n",
        "            velocity_step = 128 // self.n_velocity_bins\n",
        "        except ZeroDivisionError:\n",
        "            velocity_step = 0\n",
        "        for sample in samples:\n",
        "            sample_start_time = next((note[0] for note in sample), 0)\n",
        "            for note in sample:\n",
        "                #reshift note start and end times to begin at zero\n",
        "                note[0] -= sample_start_time\n",
        "                note[1] -= sample_start_time\n",
        "                #delete this \n",
        "                if note[0] < 0 or note[1] < 0:\n",
        "                    raise PreprocessingError\n",
        "                if timestep:\n",
        "                    #quantize timing\n",
        "                    note[0] = (note[0] * self.sampling_rate) // 1 * timestep\n",
        "                    note[1] = (note[1] * self.sampling_rate) // 1 * timestep\n",
        "                if velocity_step:\n",
        "                    #quantize dynamics\n",
        "                    #smallest velocity is 1 (otherwise we can't hear it!)\n",
        "                    note[3] = (note[3] // velocity_step *\\\n",
        "                            velocity_step) + 1\n",
        "\n",
        "    def transpose_samples(self, samples):\n",
        "        \"\"\"\n",
        "        Transposes the pitch of a sample note by note according to a list of intervals.\n",
        "        \"\"\"\n",
        "        transposed_samples = []\n",
        "        for sample in samples:\n",
        "            for transposition in self.transpositions:\n",
        "                if transposition == 0:\n",
        "                    transposed_samples.append(sample)\n",
        "                    continue\n",
        "                transposed_sample = np.copy(sample)\n",
        "                #shift pitches in sample by transposition\n",
        "                transposed_sample[:,2] += transposition\n",
        "                #should I adjust pitches that fall out of the range of \n",
        "                #a piano's 88 keys? going to be pretty uncommon.\n",
        "                transposed_samples.append(transposed_sample)\n",
        "\n",
        "        return transposed_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_zeNeT3oWuI"
      },
      "source": [
        "# attention code\n",
        "\n",
        "class AttentionError(Exception):\n",
        "    pass\n",
        "\n",
        "class MultiheadedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Narrow multiheaded attention. Each attention head inspects a \n",
        "    fraction of the embedding space and expresses attention vectors for each sequence position as a weighted average of all (earlier) positions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, heads=8, dropout=0.1, relative_pos=True):\n",
        "\n",
        "        super().__init__()\n",
        "        if d_model % heads != 0:\n",
        "            raise AttentionError(\"Number of heads does not divide model dimension\")\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        s = d_model // heads\n",
        "        self.linears = torch.nn.ModuleList([nn.Linear(s, s, bias=False) for i in range(3)])\n",
        "        self.recombine_heads = nn.Linear(heads * s, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.max_length = 1024\n",
        "        #relative positional embeddings\n",
        "        self.relative_pos = relative_pos\n",
        "        if relative_pos:\n",
        "            self.Er = torch.randn([heads, self.max_length, s],\n",
        "                    device=d())\n",
        "        else:\n",
        "            self.Er = None\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        #batch size, sequence length, embedding dimension\n",
        "        b, t, e = x.size()\n",
        "        h = self.heads\n",
        "        #each head inspects a fraction of the embedded space\n",
        "        #head dimension\n",
        "        s = e // h\n",
        "        #start index of position embedding\n",
        "        embedding_start = self.max_length - t\n",
        "        x = x.view(b,t,h,s)\n",
        "        queries, keys, values = [w(x).transpose(1,2)\n",
        "                for w, x in zip(self.linears, (x,x,x))]\n",
        "        if self.relative_pos:\n",
        "            #apply same position embeddings across the batch\n",
        "            #Is it possible to apply positional self-attention over\n",
        "            #only half of all relative distances?\n",
        "            Er  = self.Er[:, embedding_start:, :].unsqueeze(0)\n",
        "            QEr = torch.matmul(queries, Er.transpose(-1,-2))\n",
        "            QEr = self._mask_positions(QEr)\n",
        "            #Get relative position attention scores\n",
        "            #combine batch with head dimension\n",
        "            SRel = self._skew(QEr).contiguous().view(b*h, t, t)\n",
        "        else:\n",
        "            SRel = torch.zeros([b*h, t, t], device=d())\n",
        "        queries, keys, values = map(lambda x: x.contiguous()\\\n",
        "                .view(b*h, t, s), (queries, keys, values))\n",
        "        #Compute scaled dot-product self-attention\n",
        "        #scale pre-matrix multiplication   \n",
        "        queries = queries / (e ** (1/4))\n",
        "        keys    = keys / (e ** (1/4))\n",
        "\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2))\n",
        "        scores = scores + SRel\n",
        "        #(b*h, t, t)\n",
        "\n",
        "        subsequent_mask = torch.triu(torch.ones(1, t, t, device=d()),\n",
        "                1)\n",
        "        scores = scores.masked_fill(subsequent_mask == 1, -1e9)\n",
        "        if mask is not None:\n",
        "            mask = mask.repeat_interleave(h, 0)\n",
        "            wtf = (mask == 0).nonzero().transpose(0,1)\n",
        "            scores[wtf[0], wtf[1], :] = -1e9\n",
        "\n",
        "        \n",
        "        #Convert scores to probabilities\n",
        "        attn_probs = F.softmax(scores, dim=2)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        #use attention to get a weighted average of values\n",
        "        out = torch.bmm(attn_probs, values).view(b, h, t, s)\n",
        "        #transpose and recombine attention heads\n",
        "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
        "        #last linear layer of weights\n",
        "        return self.recombine_heads(out)\n",
        "\n",
        "\n",
        "    def _mask_positions(self, qe):\n",
        "        #QEr is a matrix of queries (absolute position) dot distance embeddings (relative pos).\n",
        "        #Mask out invalid relative positions: e.g. if sequence length is L, the query at\n",
        "        #L-1 can only attend to distance r = 0 (no looking backward).\n",
        "        L = qe.shape[-1]\n",
        "        mask = torch.triu(torch.ones(L, L, device=d()), 1).flip(1)\n",
        "        return qe.masked_fill((mask == 1), 0)\n",
        "\n",
        "    def _skew(self, qe):\n",
        "        #pad a column of zeros on the left\n",
        "        padded_qe = F.pad(qe, [1,0])\n",
        "        s = padded_qe.shape\n",
        "        padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n",
        "        #take out first (padded) row\n",
        "        return padded_qe[:,:,1:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeOKIVtEoYI4"
      },
      "source": [
        "# model code\n",
        "class MusicTransformerError(Exception):\n",
        "    pass\n",
        "\n",
        "class MusicTransformer(nn.Module):\n",
        "    \"\"\"Generative, autoregressive transformer model. Train on a \n",
        "    dataset of encoded musical sequences.\"\"\"\n",
        "\n",
        "    def __init__(self, n_tokens, seq_length=None, d_model=64,\n",
        "            n_heads=4, depth=2, d_feedforward=512, dropout=0.1,\n",
        "            positional_encoding=False, relative_pos=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_tokens: number of commands/states in encoded musical sequence\n",
        "            seq_length: length of (padded) input/target sequences\n",
        "            d_model: dimensionality of embedded sequences\n",
        "            n_heads: number of attention heads\n",
        "            depth: number of stacked transformer layers\n",
        "            d_feedforward: dimensionality of dense sublayer \n",
        "            dropout: probability of dropout in dropout sublayer\n",
        "            relative_pos: (bool) if True, use relative positional embeddings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #number of commands in an encoded musical sequence\n",
        "        self.n_tokens = n_tokens\n",
        "        #embedding layer\n",
        "        self.d_model = d_model\n",
        "        self.embed = SequenceEmbedding(n_tokens, d_model)\n",
        "        #positional encoding layer\n",
        "        self.positional_encoding = positional_encoding\n",
        "        if self.positional_encoding:\n",
        "            pos = torch.zeros(5000, d_model)\n",
        "            position = torch.arange(5000).unsqueeze(1)\n",
        "            #geometric progression of wave lengths\n",
        "            div_term = torch.exp(torch.arange(0.0, d_model, 2) * \\\n",
        "                            - (math.log(10000.0) / d_model))\n",
        "\t    #even positions\n",
        "            pos[0:, 0::2] = torch.sin(position * div_term)\n",
        "            #odd positions\n",
        "            pos[0:, 1::2] = torch.cos(position * div_term)\n",
        "            #batch dimension\n",
        "            pos = pos.unsqueeze(0)\n",
        "            #move to GPU if needed\n",
        "            pos = pos.to(d())\n",
        "            self.register_buffer('pos', pos)\n",
        "        else:\n",
        "            if seq_length == None:\n",
        "                raise MusicTransformerError(\"seq_length not provided for positional embeddings\")\n",
        "            self.pos = nn.Embedding(seq_length, d_model)\n",
        "        #last layer, outputs logits of next token in sequence\n",
        "        self.to_scores = nn.Linear(d_model, n_tokens)\n",
        "        self.layers = clones(DecoderLayer(d_model, n_heads,\n",
        "            d_feedforward, dropout, relative_pos), depth)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embed(x)\n",
        "        b,t,e = x.size()\n",
        "        if self.positional_encoding:\n",
        "            positions = self.pos[:, :t, :]\n",
        "        else:\n",
        "            positions = self.pos(torch.arange(t, \n",
        "                device=d()))[None, :, :].expand(b, t, e)\n",
        "        x = x + positions\n",
        "        #another dropout layer here?\n",
        "        #pass input batch and mask through layers\n",
        "        for layer in self.layers:\n",
        "            x  = layer(x, mask)\n",
        "        #one last normalization for good measure\n",
        "        z = self.norm(x)\n",
        "        return self.to_scores(z)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, size, n_heads, d_feedforward, dropout,\n",
        "            relative_pos):\n",
        "\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiheadedAttention(size, n_heads,\n",
        "                dropout, relative_pos)\n",
        "        self.feed_forward = PositionwiseFeedForward(size, d_feedforward, dropout)\n",
        "        self.size = size\n",
        "        #normalize over mean/std of embedding dimension\n",
        "        self.norm1 = nn.LayerNorm(size)\n",
        "        self.norm2 = nn.LayerNorm(size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        #perform masked attention on input\n",
        "        #masked so queries cannot attend to subsequent keys\n",
        "        #Pass through sublayers of attention and feedforward.\n",
        "        #Apply dropout to sublayer output, add it to input, and norm.\n",
        "        attn = self.self_attn(x, mask)\n",
        "        x = x + self.dropout1(attn)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        ff = self.feed_forward(x)\n",
        "        x = x + self.dropout2(ff)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "class SequenceEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard embedding, scaled by the sqrt of model's hidden state size\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size):\n",
        "        super().__init__()\n",
        "        self.d_model = model_size\n",
        "        self.emb = nn.Embedding(vocab_size, model_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFWRVnvjm54S"
      },
      "source": [
        "# train code\n",
        "def batch_to_tensors(batch, n_tokens, max_length):\n",
        "    \"\"\"\n",
        "    Make input, input mask, and target tensors for a batch of seqa batch of sequences.\n",
        "    \"\"\"\n",
        "    input_sequences, target_sequences = batch\n",
        "    sequence_lengths = [len(s) for s in input_sequences]\n",
        "    batch_size = len(input_sequences)\n",
        "\n",
        "    x = torch.zeros(batch_size, max_length, dtype=torch.long)\n",
        "    #padding element\n",
        "    y = torch.zeros(batch_size, max_length, dtype=torch.long)\n",
        "\n",
        "\n",
        "    for i, sequence in enumerate(input_sequences):\n",
        "        seq_length = sequence_lengths[i]\n",
        "        #copy over input sequence data with zero-padding\n",
        "        #cast to long to be embedded into model's hidden dimension\n",
        "        x[i, :seq_length] = torch.Tensor(sequence).unsqueeze(0)\n",
        "    \n",
        "    x_mask = (x != 0)\n",
        "    x_mask = x_mask.type(torch.uint8)\n",
        "\n",
        "    for i, sequence in enumerate(target_sequences):\n",
        "        seq_length = sequence_lengths[i]\n",
        "        y[i, :seq_length] = torch.Tensor(sequence).unsqueeze(0)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        return x.cuda(), y.cuda(), x_mask.cuda()\n",
        "    else:\n",
        "        return x, y, x_mask \n",
        "\n",
        "def train(model, training_data, validation_data,\n",
        "        epochs, batch_size, batches_per_print=100, evaluate_per=1,\n",
        "        padding_index=-100, checkpoint_path=None,\n",
        "        custom_schedule=False, custom_loss=False):\n",
        "    \"\"\"\n",
        "    Training loop function.\n",
        "    Args:\n",
        "        model: MusicTransformer module\n",
        "        training_data: List of encoded music sequences\n",
        "        validation_data: List of encoded music sequences\n",
        "        epochs: Number of iterations over training batches\n",
        "        batch_size: _\n",
        "        batches_per_print: How often to print training loss\n",
        "        evaluate_per: calculate validation loss after this many epochs\n",
        "        padding_index: ignore this sequence token in loss calculation\n",
        "        checkpoint_path: (str or None) If defined, save the model's state dict to this file path after validation\n",
        "        custom_schedule: (bool) If True, use a learning rate scheduler with a warmup ramp\n",
        "        custom_loss: (bool) If True, set loss function as Cross Entropy with label smoothing\n",
        "    \"\"\"\n",
        "\n",
        "    training_start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    if custom_schedule:\n",
        "        optimizer = TFSchedule(optimizer, model.d_model)\n",
        "    \n",
        "    if custom_loss:\n",
        "        loss_function = smooth_cross_entropy\n",
        "    else:\n",
        "        loss_function = nn.CrossEntropyLoss(ignore_index=padding_index)\n",
        "    accuracy = Accuracy()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "        print(\"GPU is available\")\n",
        "    else:\n",
        "        print(\"GPU not available, CPU used\")\n",
        "\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    #pad to length of longest sequence\n",
        "    #minus one because input/target sequences are shifted by one char\n",
        "    max_length = max((len(L) \n",
        "        for L in (training_data + validation_data))) - 1\n",
        "    for e in range(epochs):\n",
        "        batch_start_time = time.time()\n",
        "        batch_num = 1\n",
        "        averaged_loss = 0\n",
        "        averaged_accuracy = 0\n",
        "        training_batches = prepare_batches(training_data, batch_size) #returning batches of a given size\n",
        "        for batch in training_batches:\n",
        "\n",
        "            #skip batches that are undersized\n",
        "            if len(batch[0]) != batch_size:\n",
        "                continue\n",
        "            x, y, x_mask = batch_to_tensors(batch, model.n_tokens, \n",
        "                    max_length)\n",
        "            y_hat = model(x, x_mask).transpose(1,2)\n",
        "\n",
        "            #shape: (batch_size, n_tokens, seq_length)\n",
        "\n",
        "            loss = loss_function(y_hat, y)\n",
        "\n",
        "            #detach hidden state from the computation graph; we don't need its gradient\n",
        "            #clear old gradients from previous step\n",
        "            model.zero_grad()\n",
        "            #compute derivative of loss w/r/t parameters\n",
        "            loss.backward()\n",
        "            #optimizer takes a step based on gradient\n",
        "            optimizer.step()\n",
        "            training_loss = loss.item()\n",
        "            training_losses.append(training_loss)\n",
        "            #take average over subset of batch?\n",
        "            averaged_loss += training_loss\n",
        "            averaged_accuracy += accuracy(y_hat, y, x_mask)\n",
        "            if batch_num % batches_per_print == 0:\n",
        "                print(f\"batch {batch_num}, loss: {averaged_loss / batches_per_print : .2f}\")\n",
        "                print(f\"accuracy: {averaged_accuracy / batches_per_print : .2f}\")\n",
        "                averaged_loss = 0\n",
        "                averaged_accuracy = 0\n",
        "            batch_num += 1\n",
        "\n",
        "        print(f\"epoch: {e+1}/{epochs} | time: {(time.time() - batch_start_time) / 60:,.0f}m\")\n",
        "        shuffle(training_data)\n",
        "\n",
        "        if (e + 1) % evaluate_per == 0:\n",
        "\n",
        "            #deactivate backprop for evaluation\n",
        "            model.eval()\n",
        "            validation_batches = prepare_batches(validation_data,\n",
        "                    batch_size)\n",
        "            #get loss per batch\n",
        "            val_loss = 0\n",
        "            n_batches = 0\n",
        "            val_accuracy = 0\n",
        "            for batch in validation_batches:\n",
        "\n",
        "                if len(batch[0]) != batch_size:\n",
        "                    continue\n",
        "\n",
        "                x, y, x_mask = batch_to_tensors(batch, model.n_tokens, \n",
        "                        max_length)\n",
        "\n",
        "                y_hat = model(x, x_mask).transpose(1,2)\n",
        "                loss = loss_function(y_hat, y)\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += accuracy(y_hat, y, x_mask)\n",
        "                n_batches += 1\n",
        "\n",
        "            if checkpoint_path is not None:\n",
        "                try:\n",
        "                    torch.save(model.state_dict(),\n",
        "                            checkpoint_path+f\"_e{e}\")\n",
        "                    print(\"Checkpoint saved!\")\n",
        "                except:\n",
        "                    print(\"Error: checkpoint could not be saved...\")\n",
        "\n",
        "            model.train()\n",
        "            #average out validation loss\n",
        "            val_accuracy = (val_accuracy / n_batches)\n",
        "            val_loss = (val_loss / n_batches)\n",
        "            validation_losses.append(val_loss)\n",
        "            print(f\"validation loss: {val_loss:.2f}\")\n",
        "            print(f\"validation accuracy: {val_accuracy:.2f}\")\n",
        "            shuffle(validation_data)\n",
        "\n",
        "    return training_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcXsm8G6nmhw"
      },
      "source": [
        "# run code\n",
        "def main():\n",
        "    # parser = argparse.ArgumentParser(\"Script to train model on a GPU\")\n",
        "    # parser.add_argument(\"--checkpoint\", type=str, default=None,\n",
        "    #         help=\"Optional path to saved model, if none provided, the model is trained from scratch.\")\n",
        "    # parser.add_argument(\"--n_epochs\", type=int, default=5,\n",
        "    #         help=\"Number of training epochs.\")\n",
        "    # args = parser.parse_args()\n",
        "    args = {}\n",
        "    args['checkpoint'] = None\n",
        "    args['--n_epochs'] = 5\n",
        "    \n",
        "    sampling_rate = 125\n",
        "    n_velocity_bins = 32\n",
        "    seq_length = 1024\n",
        "    n_tokens = 256 + sampling_rate + n_velocity_bins\n",
        "    transformer = MusicTransformer(n_tokens, seq_length, \n",
        "            d_model = 64, n_heads = 8, d_feedforward=256, \n",
        "            depth = 4, positional_encoding=True, relative_pos=True)\n",
        "\n",
        "    if args['checkpoint'] is not None:\n",
        "        state = torch.load(args['checkpoint'])\n",
        "        transformer.load_state_dict(state)\n",
        "        print(f\"Successfully loaded checkpoint at {args['checkpoint']}\")\n",
        "    #rule of thumb: 1 minute is roughly 2k tokens\n",
        "    \n",
        "    pipeline = PreprocessingPipeline(input_dir=\"data\", stretch_factors=[0.975, 1, 1.025],\n",
        "            split_size=30, sampling_rate=sampling_rate, n_velocity_bins=n_velocity_bins,\n",
        "            transpositions=range(-2,3), training_val_split=0.9, max_encoded_length=seq_length+1,\n",
        "                                    min_encoded_length=257)\n",
        "    pipeline_start = time.time()\n",
        "    pipeline.run()\n",
        "    runtime = time.time() - pipeline_start\n",
        "    print(f\"MIDI pipeline runtime: {runtime / 60 : .1f}m\")\n",
        "\n",
        "    today = datetime.date.today().strftime('%m%d%Y')\n",
        "    checkpoint = f\"saved_models/tf_{today}\"\n",
        "\n",
        "    training_sequences = pipeline.encoded_sequences['training']\n",
        "    validation_sequences = pipeline.encoded_sequences['validation']\n",
        "    \n",
        "    batch_size = 16\n",
        "    \n",
        "    train(transformer, training_sequences, validation_sequences,\n",
        "               epochs = args['n_epochs'], evaluate_per = 1,\n",
        "               batch_size = batch_size, batches_per_print=100,\n",
        "               padding_index=0, checkpoint_path=checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM8eObDJm0CU"
      },
      "source": [
        "# **Run Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RiFxt-rsfBv"
      },
      "source": [
        "# code to reference \n",
        "# https://github.com/chathasphere/pno-ai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT3c1bKFaTNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7bf9d6-b5b5-42f0-d2f9-9f8caae38706"
      },
      "source": [
        "# testing the reference code\n",
        "! git clone https://github.com/chathasphere/pno-ai"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pno-ai'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 340 (delta 53), reused 66 (delta 31), pack-reused 246\u001b[K\n",
            "Receiving objects: 100% (340/340), 99.72 KiB | 9.06 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "godDSl3L3bCl",
        "outputId": "25e16a6a-9d19-4f53-a286-51d4b711d74c"
      },
      "source": [
        "! cd pno-ai/ && wget https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip && unzip -q maestro-v2.0.0-midi.zip -d data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-30 21:21:49--  https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59243107 (56M) [application/zip]\n",
            "Saving to: ‘maestro-v2.0.0-midi.zip’\n",
            "\n",
            "maestro-v2.0.0-midi 100%[===================>]  56.50M  70.7MB/s    in 0.8s    \n",
            "\n",
            "2021-03-30 21:21:50 (70.7 MB/s) - ‘maestro-v2.0.0-midi.zip’ saved [59243107/59243107]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0edLukL4AgH"
      },
      "source": [
        "# removing some of the data to allow it to run on collab\n",
        "! cd pno-ai/data/maestro-v2.0.0/ && rm -rf 2004/ && rm -rf 2006 && rm -rf 2008 && rm -rf 2009"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmA8ys9jamJ8",
        "outputId": "e13143d2-b4ad-403a-d7e4-d949b81d6b5b"
      },
      "source": [
        "%cd pno-ai/\n",
        "! python3 run.py\n",
        "# main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'pno-ai/'\n",
            "/content/pno-ai\n",
            "Parsing 129 midi files in /content/pno-ai/data/maestro-v2.0.0/2015...\n",
            "Could not parse MIDI-Unprocessed_R1_D1-9-12_mid--AUDIO-from_mp3_09_R1_2015_wav--2.midi\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 59, in <module>\n",
            "    main()\n",
            "  File \"run.py\", line 40, in main\n",
            "    pipeline.run()\n",
            "  File \"/content/pno-ai/preprocess/pipeline.py\", line 75, in run\n",
            "    midis = self.parse_files(chdir=True) \n",
            "  File \"/content/pno-ai/preprocess/pipeline.py\", line 116, in parse_files\n",
            "    pretty_midis += self.parse_files()\n",
            "  File \"/content/pno-ai/preprocess/pipeline.py\", line 116, in parse_files\n",
            "    pretty_midis += self.parse_files()\n",
            "  File \"/content/pno-ai/preprocess/pipeline.py\", line 128, in parse_files\n",
            "    print(\"Could not parse {}\".format(m))\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LXj6m_uuYoV"
      },
      "source": [
        "# **SetUp** **Imports**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SSe8bkuCFcN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di4X8T-iUfC7"
      },
      "source": [
        "! git clone https://github.com/czhuang/JSB-Chorales-dataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEHjjM99ui3_"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr-3FclgUYoc"
      },
      "source": [
        "# import pickle\n",
        "# import numpy as np\n",
        "# with open('JSB-Chorales-dataset/jsb-chorales-16th.pkl', 'rb') as p:\n",
        "#     data = pickle.load(p, encoding=\"latin1\")\n",
        "# test_data = data['test']\n",
        "# train_data = data['train']\n",
        "# valid_data = data['valid']\n",
        "\n",
        "# print(valid_data)\n",
        "# # creating the dataloaders\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cn8-gH-h0d_"
      },
      "source": [
        "# Using maestro data\n",
        "! cd pno-ai/\n",
        "from preprocess import PreprocessingPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269DDLs3uuF_"
      },
      "source": [
        "# LSMT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpPMedDbwdxj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnftc3itvMxx"
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A regular one layer LSTM classifier using LSTMCell -> FFNetwork structure\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, label_size, device=torch.device(\"cuda\"), dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTMCell(input_dim, hidden_dim)\n",
        "        self.hidden2ff = nn.Linear(hidden_dim,  int(np.sqrt(hidden_dim)))\n",
        "        self.ff2label = nn.Linear(int(np.sqrt(hidden_dim)), label_size)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.device = device\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0001)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_normal_(param)\n",
        "        for name, param in self.hidden2ff.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0001)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_normal_(param)\n",
        "        for name, param in self.ff2label.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0001)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_normal_(param)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        hs = torch.zeros(x.size(0), self.hidden_dim).to(self.device)\n",
        "        cs = torch.zeros(x.size(0), self.hidden_dim).to(self.device)\n",
        "\n",
        "        for i in range(x.size()[1]):\n",
        "            hs, cs = self.lstm(x[:, i], (hs, cs))\n",
        "\n",
        "        hs = self.dropout(hs)\n",
        "        hs = self.hidden2ff(hs)\n",
        "        return self.sigmoid(self.ff2label(hs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR8_5n3MucxE"
      },
      "source": [
        "#Longformer code "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzooQSFZZCWm"
      },
      "source": [
        "Longformer Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqFvJdNba-nV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fYl6Wk3XNjP"
      },
      "source": [
        "changing attention "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t33XbhbXIqM"
      },
      "source": [
        "optimizing initialization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxT8rXBDwfRV"
      },
      "source": [
        "## Transform Base Model"
      ]
    }
  ]
}